# Data Dialogue Template

## Session Information
Date: October 15, 2024
Participants: Math Department Team (6 members)
Focus Area: Year 10 Mathematics Assessment Results
Time Allocated: 30-40 minutes

## Roles
Facilitator: Sarah Chen
Timekeeper: Michael Brown
Presenter: James Wilson
Recorder: Lisa Thompson

## Pre-Dialogue Setup
### Data Overview
Type of Data: Term 3 Assessment Results + Student Survey
Time Period: July-September 2024
Context: Post-curriculum modification implementation

### Materials Checklist
[x] Data sets organized
[x] Visual displays prepared
[x] Timer
[x] Recording method ready
[x] Copies of protocol

## 1. Data Presentation (5 minutes)
### Data Sources
1. Term 3 Assessment Results (n=150 students)
2. Student Feedback Survey (85% response rate)
3. Teacher Observation Notes

### Key Facts (No Interpretation)
- Class average: 72%
- Completion rate: 95%
- Survey response rate: 85%
- Problem-solving section average: 65%
- Computational skills section average: 78%

### Context Notes
- New problem-solving approach implemented
- Additional support sessions available
- Modified assessment format

## 2. Clarifying Questions (5 minutes)
### Questions Asked
1. How many students attended support sessions?
2. What was last term's completion rate?
3. Were accommodations provided as usual?

### Additional Information Needed
- Previous term's problem-solving scores
- Support session attendance data
- Specific question completion rates

## 3. Observations (10 minutes)
### Patterns Noticed
- Higher completion rate than previous assessments
- Lower scores in problem-solving section
- Strong performance in computational skills
- Mixed feedback on new approach

### Notable Points
- Significant variation between classes
- High engagement in support sessions
- Improved scores for previously struggling students

### Surprising Elements
- Large gap between computation and problem-solving
- Higher completion rate despite complexity
- Positive feedback on group work

### Missing Information
- Individual question analysis
- Comparison with previous cohorts
- Student interview data

## 4. Interpretation (10 minutes)
### Key Insights
1. New approach showing mixed results
2. Strong basic skills not translating to problem-solving
3. Support sessions appear effective for attendees

### Potential Factors
- Implementation consistency across classes
- Student familiarity with new approach
- Time management in assessment
- Support session accessibility

### Assumptions to Test
- Connection between support and performance
- Impact of new teaching approach
- Effect of assessment format changes

### Additional Data Needed
- Detailed question-by-question analysis
- Student interviews about problem-solving
- Teacher implementation surveys

## 5. Implications (5 minutes)
### Action Items
| Action | Timeline | Responsible | Resources Needed |
|--------|----------|-------------|------------------|
| Create problem-solving guide | 2 weeks | Sarah/James | Example problems |
| Analyze support session data | 1 week | Michael | Attendance records |
| Student focus groups | 3 weeks | Lisa | Interview protocol |

### Next Steps
1. Schedule student focus groups
2. Review problem-solving instruction
3. Modify support session structure

### Follow-up Data Needs
- Student focus group responses
- Individual question analysis
- Support session effectiveness data

## Notes and Reflections
Need to consider differentiation strategies
Consider peer tutoring program
Review assessment format

## Next Meeting
Date: October 29, 2024
Focus: Review of implementation changes
Data to Prepare: Focus group results, support session analysis